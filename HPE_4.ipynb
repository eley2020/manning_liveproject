{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HPE-4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMu2YYShMZClZx2TsRFZs0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eley2020/manning_liveproject/blob/master/HPE_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlnXsZp5RURt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "dddcaa11-088e-488e-f6c1-17a168f03265"
      },
      "source": [
        "# This code downloads the coco dataset from Amazon S3 in parallel.\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "import multiprocessing\n",
        "import subprocess\n",
        "#files = [ 'train2017.zip']#,'val2017.zip', 'annotations_trainval2017.zip']\n",
        "files = [ 'val2017.zip', 'annotations_trainval2017.zip']\n",
        "\n",
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "def download_and_unzip_from_s3(file_name, bucket_name='fast-ai-coco'):\n",
        "    print(\"Downloading\", file_name)\n",
        "    s3.download_file(bucket_name, file_name, file_name)\n",
        "    print(\"Finished downloading\", file_name, \". Starting to unzip.\")\n",
        "    subprocess.run([\"unzip\", file_name])\n",
        "    print(\"Finished unzipping\", file_name)\n",
        "\n",
        "# Download in parallel\n",
        "num_cpus = multiprocessing.cpu_count()\n",
        "with multiprocessing.Pool(num_cpus) as p:\n",
        "    p.map(download_and_unzip_from_s3, files)\n",
        "\n",
        "print(\"Done transferring all datasets\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading annotations_trainval2017.zip\n",
            "Downloading val2017.zip\n",
            "Finished downloading annotations_trainval2017.zip . Starting to unzip.\n",
            "Finished downloading val2017.zip . Starting to unzip.\n",
            "Finished unzipping val2017.zip\n",
            "Finished unzipping annotations_trainval2017.zip\n",
            "Done transferring all datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyxsBqKuSENS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t9x6m7aSU8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the meta data of the validation set\n",
        "import json\n",
        "file_name = \"annotations//person_keypoints_val2017.json\"\n",
        "with open(file_name, 'r') as json_raw:\n",
        "    meta = json.load(json_raw)\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import math\n",
        "from scipy.io import loadmat\n",
        "from matplotlib.pyplot import imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4s4LofkSZVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "#from matplotlib.pyplot import imshow\n",
        "from matplotlib import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import cv2\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "class HumanPoseDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, meta_path,train= False, transform=None):\n",
        "        \n",
        "        self.meta = self.load_meta(meta_path)\n",
        "        self.train = train\n",
        "        \n",
        "        self.img_annotations = self.filter_annotations(self.meta[\"annotations\"])\n",
        "        #self.img_info = meta[\"images\"]\n",
        "        #self.img_catogory = meta[\"categories\"][0][\"keypoints\"]\n",
        "        self.mean = [0.485, 0.456, 0.406]\n",
        "        self.std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    def load_meta(self,path):\n",
        "      \n",
        "      with open(path, 'r') as json_raw:\n",
        "          meta = json.load(json_raw)\n",
        "      return meta\n",
        "\n",
        "    def filter_annotations(self,meta):\n",
        "      min_width = 48 #192 # 12\n",
        "      min_height = 64 # 256 # 16\n",
        "      print(\"LEN before filter\",len(meta))\n",
        "      annot = filter(lambda x: x[\"iscrowd\"] == 0 and \n",
        "                    x['bbox'][2]>= min_width and x['bbox'][3]>=min_height and\n",
        "                    any(map(lambda y: y>0, x[\"keypoints\"][2:-1:3])) # visible              \n",
        "                    ,meta)\n",
        "      annot = list(annot)\n",
        "      \"\"\"\n",
        "      cleaned_annot = []\n",
        "      for sample in meta:\n",
        "        no_crowd = sample[\"iscrowd\"] == 0\n",
        "        start_x,start_y, w,h = sample['bbox']\n",
        "        not_too_small = w>= min_width and h>=min_height\n",
        "        inside_box = False\n",
        "        #print(sample[\"keypoints\"])\n",
        "        keypoints = sample[\"keypoints\"]\n",
        "        for i in range(len(keypoints)//3):\n",
        "          \n",
        "          keypoint = (keypoints[i*3],keypoints[i*3+1])\n",
        "          kx,ky = keypoint\n",
        "          if not (start_x<kx and kx < (start_x+w) and start_y<ky and ky < (start_y+h)):\n",
        "            inside_box = True\n",
        "            break\n",
        "        if all([no_crowd, not_too_small, inside_box]):\n",
        "          cleaned_annot.append(sample)\n",
        "        \"\"\"\n",
        "      print(\"AFTER\", len(annot))\n",
        "      return annot #cleaned_annot\n",
        "\n",
        "    def __len__(self):\n",
        "        #print(len(self.img_annotations), \"anot\")\n",
        "        return len(self.img_annotations)\n",
        "\n",
        "    def get_image_name(self,annot):\n",
        "      \n",
        "      img_file = str(annot[\"image_id\"])\n",
        "      img_file = img_file.zfill(12) +\".jpg\" #img_info[\"file_name\"]\n",
        "      if self.train:\n",
        "        return \"train2017//\"+img_file\n",
        "      else:   \n",
        "        return \"val2017//\"+img_file\n",
        "\n",
        "    def resize_image(self,img, target_width=192, target_height=256):\n",
        "      img_resized = cv2.resize(img,(target_width,target_height))   \n",
        "      return img_resized\n",
        "\n",
        "    def crop_image(self,img, upper_left_corner, size):\n",
        "      #print(upper_left_corner,size, img.shape)\n",
        "      start_x, start_y = upper_left_corner\n",
        "      w,h = size\n",
        "      #print(\"CROP\",int(start_y),(int(start_y+h-1)),int(start_x),int(start_x+w-1), img.shape)\n",
        "      img_cropped = img[int(start_y):(int(start_y+h)),int(start_x):int(start_x+w),:]\n",
        "      #print(\"CROP\", img.shape)\n",
        "      return img_cropped\n",
        "\n",
        "    def adjust_keypoint(self,keypoint, start, scale):\n",
        "      keypoint_x, keypoint_y= keypoint\n",
        "      start_x,start_y = start\n",
        "      sx,sy = scale\n",
        "      x = (keypoint_x-start_x)*sx\n",
        "      y = (keypoint_y-start_y)*sy\n",
        "      return x,y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        annot = self.img_annotations[idx]\n",
        "        keypoints = annot[\"keypoints\"]\n",
        "\n",
        "        img_name = self.get_image_name(annot)\n",
        "        image = io.imread(img_name)\n",
        "        #\n",
        "        #print(\"IDX\",idx)\n",
        "        if len(image.shape) <3:\n",
        "          # thats a freakin gray image\n",
        "          image = np.dstack((image, image,image))\n",
        "        #plt.imshow(image)\n",
        "        target_width = 192\n",
        "        target_height = 256\n",
        "\n",
        "        #print(annot[\"bbox\"],\"box\")\n",
        "        start_x,start_y, w,h = annot[\"bbox\"]\n",
        "        #print(\"box\",annot[\"bbox\"])\n",
        "        #print(image.shape,\"ishape\")\n",
        "        img_crop = self.crop_image(image, (start_x,start_y),(w,h))\n",
        "        #print(\"crop\",img_crop.shape)\n",
        "        #plt.imshow(img_crop)\n",
        "        img_crop = self.resize_image(img_crop,target_width,target_height)\n",
        "        \n",
        "        img = img_crop / 255.0\n",
        "        img = (img-self.mean)/self.std\n",
        "        # imgshape (256, 192, 3)\n",
        "        #print(\"img\",img.shape)\n",
        "        img = np.transpose(img,[2,0,1]) # channel first\n",
        "\n",
        "        #img_batch = torch.Tensor([img, img])\n",
        "        #print(img_batch.shape)\n",
        "        #img_batch = img_batch.permute([0,3,2,1])\n",
        "\n",
        "        sx = target_width/w \n",
        "        sy = target_height/h\n",
        "        #print(\"sxsy\",sx,sy)\n",
        "\n",
        "        validity = annot[\"keypoints\"][2::3]\n",
        "\n",
        "        validity = np.array(validity)\n",
        "        validity = validity > 0\n",
        "        validity = validity.astype(np.int)\n",
        "        #print(validity.shape)\n",
        "\n",
        "        heatmaps = np.zeros((17,64,48))\n",
        "\n",
        "        keypoints_adjusted = []\n",
        "        for i in range(len(keypoints)//3):\n",
        "          \n",
        "          keypoint = (keypoints[i*3],keypoints[i*3+1])\n",
        "          visible = keypoints[i*3+2]\n",
        "          #print(keypoint)\n",
        "          if visible:\n",
        "            x,y = self.adjust_keypoint(keypoint,(start_x,start_y),(sx,sy))\n",
        "            #print(\"XY\", x,y,int(y//4),int(x//4),sx,sy )\n",
        "            #not (start_x<kx and kx < (start_x+w) and start_y<ky and ky < (start_y+h))\n",
        "            kx , ky = keypoint\n",
        "            #if kx< start_x or ky < start_y or kx> (start_x+w) or ky > (start_y +h):\n",
        "            if not (start_x<kx and kx < (start_x+w) and start_y<ky and ky < (start_y+h)):\n",
        "              validity[i] = 0\n",
        "              #print(\"FAIL\", start_x, kx, start_x+w, start_y,y, start_y+h)\n",
        "            else:\n",
        "              \n",
        "              #if (y//4 >= 64 or x//4 >=47):\n",
        "              #  print(\"fail\",idx,x,y)\n",
        "              heatmaps[i,int(y//4),int(x//4)] = 1\n",
        "              heatmaps[i,:,:] =  gaussian_filter(heatmaps[i,:,:], sigma=2)\n",
        "              #print(heatmaps[i,:,:].min(),heatmaps[i,:,:].max())\n",
        "              heatmaps[i,:,:] -= heatmaps[i,:,:].min() \n",
        "              heatmaps[i,:,:] /= heatmaps[i,:,:].max()\n",
        "            #print(\"N\",heatmaps[i,:,:].min(),heatmaps[i,:,:].max())\n",
        "            #plt.matshow(heatmaps[i,:,:])\n",
        "            #print(meta[\"categories\"][0][\"keypoints\"][i])\n",
        "          else:\n",
        "            x,y = (-1,-1)\n",
        "          keypoints_adjusted.append(x)\n",
        "          keypoints_adjusted.append(y)\n",
        "          \n",
        "        \"\"\"\n",
        "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "        landmarks = np.array([landmarks])\n",
        "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "        sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        \"\"\"\n",
        "        return torch.Tensor(img), torch.Tensor(heatmaps), torch.Tensor(validity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHd1pMM1SkX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f4b7853f-6a85-4fbf-9292-d7e08fdb9b1f"
      },
      "source": [
        "data_val = HumanPoseDataset(meta_path=\"annotations//person_keypoints_val2017.json\")\n",
        "data_train = HumanPoseDataset(meta_path=\"annotations//person_keypoints_train2017.json\", train=True)\n",
        "#data_train = HumanPoseDataset(meta_path=\"annotations//person_keypoints_val2017.json\", train=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LEN before filter 11004\n",
            "AFTER 4795\n",
            "LEN before filter 262465\n",
            "AFTER 112208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIv4iV1YSoGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(data_val)):\n",
        "  print(i)\n",
        "  _, labels, _ = data_val[i]\n",
        "  \n",
        "  # h in range(labels.shape[0]):\n",
        "  #  plt.matshow(labels[h,:,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg3LUidTS1DF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example = 0\n",
        "img_info = meta[\"images\"][example]\n",
        "img_annotation = meta[\"annotations\"][example]\n",
        "img_catogory = meta[\"categories\"][0][\"keypoints\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRHX62tLS4wT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #block\n",
        "        self.conv_1 = nn.Conv2d(in_channels = 3, out_channels=64,kernel_size=7,stride=2,padding=3)\n",
        "        #nn.init.normal_(self.conv_1.weight, std=0.001)\n",
        "        #nn.init.constant_(self.conv_1.bias,0)\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(64) # same as out_channels before layer \n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.pool_1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "\n",
        "        self.conv_2 = nn.Conv2d(in_channels = 64, out_channels=128,kernel_size=5,stride=1,padding=2)\n",
        "        #nn.init.normal_(self.conv_2.weight, std=0.001)\n",
        "        #nn.init.constant_(self.conv_2.bias,0)\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(128) # same as out_channels before layer \n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.pool_2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv_3 = nn.Conv2d(in_channels= 128, out_channels=256,kernel_size=5,stride=1,padding=2)\n",
        "        #nn.init.normal_(self.conv_3.weight, std=0.001)\n",
        "        #nn.init.constant_(self.conv_3.bias,0)\n",
        "        self.batch_norm_3 = nn.BatchNorm2d(256) # same as out_channels before layer \n",
        "        self.relu_3 = nn.ReLU()\n",
        "        self.pool_3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # lazy explanition in the workflow...\n",
        "        # right before\n",
        "        self.up1 = nn.ConvTranspose2d(256, 256, 4, stride=2,padding=1)\n",
        "        nn.init.normal_(self.up1.weight, std=0.001)\n",
        "        self.batch_norm_4 = nn.BatchNorm2d(256) # same as out_channels before layer \n",
        "        self.relu_4 = nn.ReLU()\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 256, 4, stride=2,padding=1)\n",
        "        nn.init.normal_(self.up2.weight, std=0.001)\n",
        "        self.batch_norm_5 = nn.BatchNorm2d(256) # same as out_channels before layer \n",
        "        self.relu_5 = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Conv2d(in_channels= 256, out_channels=17,kernel_size=1,stride=1,padding=0) #nn.Linear(24 * 4 * 4, 10)# 24 chans x 32//(2*2*2)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "        #nn.init.normal_(self.fc1.weight, std=0.001)\n",
        "        #nn.init.constant_(self.fc1.bias,0)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_1(x)\n",
        "        x = self.batch_norm_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.pool_1(x)\n",
        "\n",
        "        x = self.conv_2(x)\n",
        "        x = self.batch_norm_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.pool_2(x)\n",
        "\n",
        "        x = self.conv_3(x)\n",
        "        x = self.batch_norm_3(x)\n",
        "        x = self.relu_3(x)\n",
        "        x = self.pool_3(x)\n",
        "\n",
        "        x = self.up1(x)\n",
        "        x = self.batch_norm_4(x)\n",
        "        x = self.relu_4(x)\n",
        "\n",
        "        x = self.up2(x)\n",
        "        x = self.batch_norm_5(x)\n",
        "        x = self.relu_5(x)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        x = self.fc1(x)#x.view(-1, 24 * 4 * 4)\n",
        "        x = self.sig(x)\n",
        "        \n",
        "        #x = F.relu(self.fc1(x))\n",
        "        #x = F.relu(self.fc2(x))\n",
        "        #x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}